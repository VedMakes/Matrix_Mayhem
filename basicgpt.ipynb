{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4406386,"sourceType":"datasetVersion","datasetId":1733714}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tiktoken as tk\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T12:07:16.342034Z","iopub.execute_input":"2025-08-18T12:07:16.342262Z","iopub.status.idle":"2025-08-18T12:07:22.821551Z","shell.execute_reply.started":"2025-08-18T12:07:16.342238Z","shell.execute_reply":"2025-08-18T12:07:22.820687Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Model Hyperparameters\n\n- `d_model = 256`  \n  - Dimension of token embeddings and hidden representations.  \n  - All attention computations, residuals, and FFN inputs/outputs use this size.  \n\n- `num_heads = 8`  \n  - Number of attention heads in each MultiHeadAttention layer.  \n  - Each head works in a subspace of size `head_dim = d_model / num_heads = 32`.  \n\n- `d_ff = 1024`  \n  - Hidden size of the feed-forward network inside each Transformer block.  \n  - Typically 4x `d_model` in GPT architectures → expansion-bottleneck style.  \n\n- `num_layers = 4`  \n  - Number of stacked TransformerBlocks in the model.  \n  - More layers → more capacity, deeper contextual understanding, but slower to train.  \n\n- `max_len = 256`  \n  - Maximum sequence length the positional embeddings can handle.  \n  - Sequences longer than this will need truncation or extension of positional embeddings.  \n\n- `vocab_size = 50257`  \n  - Size of GPT-2’s BPE tokenizer vocabulary (includes special tokens).  \n  - Needed for `EmbeddingLayer` and final output `head`.  \n\n- `head_dim = 32`  \n  - Dimension of each attention head (`d_model / num_heads`).  \n  - Scales the attention scores: smaller head_dim → less expressive, larger → more compute.\n\n- `lr = 2e-4`\n  - learning rate of the optimization algorithm (AdamW here)\n  - increase or decrease according to the dataset and needs.\n\n- `epochs = 100`\n  - Total epochs for training the model\n  - decrease for faster training but poorer results","metadata":{}},{"cell_type":"code","source":"d_model = 256\nnum_heads = 8\nd_ff = 1024\nnum_layers = 4\nmax_len = 256\nvocab_size = 50257\nhead_dim = 32\nlr = 2e-4\nepochs = 10000\ndevice = 'cuda'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T12:07:22.823871Z","iopub.execute_input":"2025-08-18T12:07:22.824293Z","iopub.status.idle":"2025-08-18T12:07:22.829400Z","shell.execute_reply.started":"2025-08-18T12:07:22.824270Z","shell.execute_reply":"2025-08-18T12:07:22.828214Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Tokenizing\n- I use OpenAI's tiktoken library here mainly because its faster\n- however, if access to GPUs is restricted, custom tokenizer from HuggingFace should be used for faster results\n- The tiktoken tokenizer was also used for GPT-2, and has been sufficiently battle-tested, hence using.","metadata":{}},{"cell_type":"code","source":"encoder = tk.get_encoding(\"gpt2\")\nEOT= encoder.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})  ### [50256]\n\nenc = encoder.encode(\"Hello world\", allowed_special={\"<|endoftext|>\"})\n\ndec = encoder.decode(enc)\n\nprint(enc)\n\nprint(dec)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T12:07:22.830464Z","iopub.execute_input":"2025-08-18T12:07:22.830808Z","iopub.status.idle":"2025-08-18T12:07:24.360416Z","shell.execute_reply.started":"2025-08-18T12:07:22.830782Z","shell.execute_reply":"2025-08-18T12:07:24.359698Z"}},"outputs":[{"name":"stdout","text":"[15496, 995]\nHello world\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Data Loader\n\n### `enc(s: str) -> list[int]`\n- Takes a string `s` and converts it into a list of token ids using the GPT-2 encoder.  \n- Keeps special tokens like `<|endoftext|>` intact.  \n- Returns a Python list of integers representing tokens.  \n- Simple wrapper around `encoder.encode()` for consistancy.  \n- Quick and light, no device or batch handlng.  \n\n\n### `dec(ids: list[int]) -> str`\n- Converts a list of token ids back into human readable text.  \n- Wrapper around `encoder.decode()`.  \n- Useful for seeing what the model actualy “says”.  \n- Works with special tokens, no filtering by default.  \n- Takes a list of ints, returns a single string.  \n\n### `build_ids(data, add_eot: bool = True) -> torch.Tensor`\n- Turns a string or list of strings into a 1D `LongTensor` of token ids.  \n- Automatically adds an `<|endoftext|>` token after each string if `add_eot=True`.  \n- Can handle both single string and list of strings transparently.  \n- Concatenates all tokens into one flat tensor for trainng.  \n- Returns a tensor ready to be sliced into batches for the model.  \n\n### `batch_loader(raw_dataset, T: int = 64, B: int = 8, device: str = \"cuda\")`\n- Creates random batches of sequences for next-token prediction.  \n- `x` is the input sequence, `y` is the target sequence shifted by 1 token.  \n- Samples `B` starting points from the dataset, each of length `T`.  \n- Moves the batch to the specified `device` in one go for speed.  \n- Raises error if dataset is too small for the requested sequnce length.  \n","metadata":{}},{"cell_type":"code","source":"def enc(s: str) -> list[int]:\n    return encoder.encode(s, allowed_special={\"<|endoftext|>\"})\ndef dec(ids: list[int]) -> str:\n    return encoder.decode(ids)\n\n\ndef build_ids(data, add_eot: bool = True) -> torch.Tensor:\n    \n    \n    \"\"\"\n    data: str | list[str] \n    returns: 1D LongTensor of token ids\n    \"\"\"\n\n    \n    if isinstance(data, str):\n        txts = [data]\n    else:\n        txts = list(data)\n\n    buf = []\n    for s in txts:\n        buf.extend(enc(s))\n        if add_eot:\n            buf.extend(EOT)\n    return torch.tensor(buf, dtype=torch.long)\n\n\n\n\n\n@torch.no_grad() ## Saves memory\ndef batch_loader(raw_dataset, T: int = 64, B: int = 8, device: str = \"cuda\"):\n    \n    \n    \"\"\"\n    ids: 1D LongTensor [N]\n    T:   sequence length (context size)\n    B:   batch size\n    returns: x,y each [B, T] on `device`\n    \"\"\"\n\n    ## Encodes the dataset\n    ids = build_ids(raw_dataset, add_eot = True)\n\n    \n    ###Check if token sequence is too small\n    N = ids.size(0)\n    if N <= T + 1:\n        raise ValueError(f\"Need more tokens (got {N}) than T+1 ({T+1}).\")\n\n    \n    # sample B starting positions\n    i = torch.randint(0, N - T - 1, (B,))\n    \n    \n    # gather slices (CPU) then move once (faster than so many tiny transfers)\n    x = torch.stack([ids[j:j+T]     for j in i], dim=0)\n    y = torch.stack([ids[j+1:j+T+1] for j in i], dim=0)\n    return x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T12:07:24.361565Z","iopub.execute_input":"2025-08-18T12:07:24.361824Z","iopub.status.idle":"2025-08-18T12:07:24.370167Z","shell.execute_reply.started":"2025-08-18T12:07:24.361804Z","shell.execute_reply":"2025-08-18T12:07:24.369244Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Test/Demo","metadata":{}},{"cell_type":"code","source":"txts = [\n    \"transformers are spicy attention machines.\",\n    \"attention is all you need, allegedly.\",\n    \"lets build the beast today.\"\n]\n\nx, y = batch_loader(txts, T=6, B=8, device=\"cpu\")\nprint(x.shape, y.shape)           #torch.Size([8, 64]) torch.Size([8, 64])\n\nprint(x)\n\nfor i in range(0,8):\n    print(dec(x[i].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T12:07:24.370995Z","iopub.execute_input":"2025-08-18T12:07:24.371270Z","iopub.status.idle":"2025-08-18T12:07:24.450174Z","shell.execute_reply.started":"2025-08-18T12:07:24.371242Z","shell.execute_reply":"2025-08-18T12:07:24.449339Z"}},"outputs":[{"name":"stdout","text":"torch.Size([8, 6]) torch.Size([8, 6])\ntensor([[ 7910,    13, 50256,  5289,  1382,   262],\n        [  364,   389, 26880,  3241,  8217,    13],\n        [   11,  7910,    13, 50256,  5289,  1382],\n        [  477,   345,   761,    11,  7910,    13],\n        [  389, 26880,  3241,  8217,    13, 50256],\n        [  761,    11,  7910,    13, 50256,  5289],\n        [ 7910,    13, 50256,  5289,  1382,   262],\n        [35636,   364,   389, 26880,  3241,  8217]])\n allegedly.<|endoftext|>lets build the\ners are spicy attention machines.\n, allegedly.<|endoftext|>lets build\n all you need, allegedly.\n are spicy attention machines.<|endoftext|>\n need, allegedly.<|endoftext|>lets\n allegedly.<|endoftext|>lets build the\ntransformers are spicy attention machines\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Embedding and Attention\n\n### `EmbeddingLayer`\n- Combines token embeddings and positional embeddings into one vector per token.  \n- `self.tok_embed` maps each token id to a learnable `d_model` dimensional vector.  \n- `self.pos_embed` assigns a learnable vector to each position in the sequence up to `max_len`.  \n- In `forward()`, we create position ids `[0,1,...,T-1]` and look up their embeddings.  \n- Returns `tok + pos` → the model can know **what the token is** and **where it is** in the sequence.  \n- Nuance: the sum `tok + pos` assumes `d_model` for both; if dimensions mismatch, PyTorch will error.  \n- Another nuance: positional embeddings are learned (unlike sinusoidal) — model has to figure out position info from scratch.  \n\n### `SingleHeadAttention`\n- Implements a single “self-attention head” from the Transformer paper.  \n- `W_q`, `W_k`, `W_v` are linear layers projecting `d_model` -> `d_k` for queries, keys, and values.  \n- `forward(x)`:\n  - Q = W_q(x), K = W_k(x), V = W_v(x)  \n  - Computes attention scores: `scores = Q K^T / sqrt(d_k)`  \n    - Divide by `sqrt(d_k)` to stabilize gradients (so softmax isn’t too peaky).  \n  - Apply `softmax` along the last dimension → each token attends to all other tokens.  \n  - Multiply `attn` with `V` → weighted sum of values gives final representation for each token.  \n- Returns `out` (transformed tokens) and `attn` (attention map for analysis/debug).  \n- Nuances:\n  - This is **full self-attention**, O(T²) complexity — slow for long sequences.  \n  - No masking here — so if you use it for autoregressive generation, you’d need to mask future tokens outside this module.  \n  - `d_k` can be smaller than `d_model`; if multihead is used, each head works in its own subspace.  \n- Fun fact: Q, K, V are all learned projections; the model decides **what to “pay attention to”** via training.  \n","metadata":{}},{"cell_type":"code","source":"class EmbeddingLayer(nn.Module):\n    def __init__(self, vocab_size, d_model, max_len=2048):\n        super().__init__()\n        self.tok_embed = nn.Embedding(vocab_size, d_model)   # token embedding\n        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n\n    def forward(self, x):\n        B, T = x.shape\n        # make position IDs: [0, 1, ..., T-1]\n        pos = torch.arange(0, T, device=x.device).unsqueeze(0)  # [1, T]\n        tok = self.tok_embed(x)       # [B, T, d_model]\n        pos = self.pos_embed(pos)     # [1, T, d_model]\n        return tok + pos              # [B, T, d_model]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T12:07:24.451029Z","iopub.execute_input":"2025-08-18T12:07:24.451663Z","iopub.status.idle":"2025-08-18T12:07:24.457078Z","shell.execute_reply.started":"2025-08-18T12:07:24.451607Z","shell.execute_reply":"2025-08-18T12:07:24.456191Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\nclass SingleHeadAttention(nn.Module):\n    def __init__(self, d_model, d_k):\n        super().__init__()\n        \n        \n        # Linear projections for Q, K, V\n        self.W_q = nn.Linear(d_model, d_k, bias=False)\n        self.W_k = nn.Linear(d_model, d_k, bias=False)\n        self.W_v = nn.Linear(d_model, d_k, bias=False)\n        \n    \n    \n    def forward(self, x):\n        # x shape: (batch_size, seq_len, d_model)\n        Q = self.W_q(x)  \n        K = self.W_k(x)  \n        V = self.W_v(x) \n        \n        \n        # Attention scores: QK^T / sqrt(d_k) jus like the goddamn paper it was hell to code aaaaaaaaaaaaa\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5)\n\n        \n        # Softmax over last dim\n        attn = F.softmax(scores, dim=-1)\n        \n        ##Weighted sum with V\n        out = torch.matmul(attn, V)  # (batch_size, seq_len, d_k)\n        return out, attn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T12:07:24.459322Z","iopub.execute_input":"2025-08-18T12:07:24.459583Z","iopub.status.idle":"2025-08-18T12:07:24.474765Z","shell.execute_reply.started":"2025-08-18T12:07:24.459555Z","shell.execute_reply":"2025-08-18T12:07:24.473835Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Test/Demo","metadata":{}},{"cell_type":"code","source":"embed = EmbeddingLayer(vocab_size, d_model, max_len)\nattn = SingleHeadAttention(d_model, head_dim)\n\n\nemb = embed(x)   # [B, T, d_model]\n\nout = attn(emb)  # [B, T, d_model]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T12:07:24.475538Z","iopub.execute_input":"2025-08-18T12:07:24.475805Z","iopub.status.idle":"2025-08-18T12:07:24.663572Z","shell.execute_reply.started":"2025-08-18T12:07:24.475786Z","shell.execute_reply":"2025-08-18T12:07:24.662596Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# MultiHeadAttention\n\n### `MultiHeadAttentionOld`\n- Implements multi-head attention **naively by creating independent SingleHeadAttention objects** for each head.  \n- `num_heads` separate SingleHeadAttention instances, each mapping `d_model -> d_head`.  \n- Forward pass:\n  - Loops through each head, runs forward separately: **slow, non-batched**.  \n  - Concatenates outputs along feature dimension → shape `(B, L, d_model)`.  \n  - Final linear `W_o` mixes all heads back together.  \n- Nuances:\n  - Easier to understand conceptually (each head is a standalone attention).  \n  - **Extremely slow for long sequences** because each head is computed sequentially.  \n  - Harder to optimize on GPU due to many small matrix multiplies.  \n  - Parameter count higher if you naively duplicate weights per head.  \n\n### `MultiHeadAttentionNew`\n- Implements multi-head attention **efficiently using one big linear projection** per Q, K, V.  \n- `d_model` is split into `num_heads` heads, each of size `d_head = d_model // num_heads`.  \n- Forward pass:\n  - Project Q, K, V all at once via `self.W_q`, `self.W_k`, `self.W_v`.  \n  - Reshape to `(B, num_heads, L, d_head)` to separate heads.  \n  - Compute **scaled dot-product attention** in a batched fashion: `scores = QK^T / sqrt(d_head)`.  \n  - Optional mask applied to prevent attending to certain positions (useful for autoregressive tasks).  \n  - Multiply attention weights by V, then merge heads back: `(B, L, d_model)`.  \n  - Final linear `W_o` mixes information across heads.  \n- Nuances:\n  - **Batched attention** → fast, GPU-friendly, memory efficient.  \n  - All heads share a **single projection layer** (big linear) → fewer params, better vectorization.  \n  - Supports optional `k` and `v` inputs for cross-attention style usage.  \n\n###  Key Differences\n- **Computation style:**\n  - `New` → all heads projected and computed in **one big batch** (fast, GPU optimized).  \n  - `Old` → each head computed **separately in a Python loop** (slow, memory inefficient).  \n- **Parameter sharing:**\n  - `New` → single linear layer per Q/K/V, implicitly contains all heads.  \n  - `Old` → each head has its own Q/K/V linear layers, duplicated params.  \n- **Performance:**\n  - `New` → fast, scalable, modern style used in GPT and Transformer implementations.  \n  - `Old` → slow, mainly for learning / pedagogical purposes.  \n- **Flexibility:**\n  - `New` → supports optional `k` and `v` for cross-attention.  \n  - `Old` → strictly self-attention unless you modify each head manually.  \n","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttentionOld(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n\n        self.num_heads = num_heads\n        self.d_head = d_model // num_heads\n\n        #spawn a bunch of independent heads\n        self.heads = nn.ModuleList([\n            SingleHeadAttention(d_model, self.d_head)\n            for _ in range(num_heads)\n        ])\n\n        self.W_o = nn.Linear(d_model, d_model)\n\n    \n    \n    def forward(self, q, k, v, mask=None):\n        ## run each head separately (slow as all hell so please dont do this one)\n        out_per_head = [head(q, k, v, mask) for head in self.heads]\n\n        concat = torch.cat(out_per_head, dim=-1)\n\n        return self.W_o(concat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T12:07:24.664585Z","iopub.execute_input":"2025-08-18T12:07:24.664926Z","iopub.status.idle":"2025-08-18T12:07:24.671661Z","shell.execute_reply.started":"2025-08-18T12:07:24.664896Z","shell.execute_reply":"2025-08-18T12:07:24.670762Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class MultiHeadAttentionNew(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0\n        self.num_heads = num_heads\n        self.d_head = d_model // num_heads\n\n        # Single fat-ass projections\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n\n        # Final output mixer\n        self.W_o = nn.Linear(d_model, d_model)\n\n    \n    \n    def forward(self, q, k = None, v = None, mask=None):\n        B, L, O = q.shape\n\n        if k is None:\n            k = q\n        if v is None:\n            v = q\n\n        \n        Q = self.W_q(q)  # (B, L, d_model)\n        K = self.W_k(k)\n        V = self.W_v(v)\n        \n\n        Q = Q.view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n        K = K.view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n        V = V.view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n\n        \n        # scaled dot-product attention (batched!!!!!)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_head ** 0.5)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        attn = torch.softmax(scores, dim=-1)\n\n        out = torch.matmul(attn, V)  # (B, num_heads, L, d_head)\n\n        # back to (B, L, d_model)\n        out = out.transpose(1, 2).contiguous().view(B, L, -1)\n        return self.W_o(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T12:07:24.672485Z","iopub.execute_input":"2025-08-18T12:07:24.672734Z","iopub.status.idle":"2025-08-18T12:07:24.682537Z","shell.execute_reply.started":"2025-08-18T12:07:24.672717Z","shell.execute_reply":"2025-08-18T12:07:24.681880Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# MLP/FFNN layer\n\n### `FeedForward`\n- Implements the **position-wise feed-forward network** used in Transformers.  \n- Two linear layers: `fc1` expands `d_model` -> `d_ff`, `fc2` projects back `d_ff` -> `d_model`.  \n- Forward pass:\n  - `fc1(x)` → expands each token vector to higher dimension (`d_ff`) for richer representation.  \n  - `F.gelu(x)` → non-linear activation, smooth version of ReLU; used in GPTs.  \n  - `fc2(x)` → projects back to original embedding size so residuals can be added.  \n  - `Dropout` applied after second layer → prevents overfitting, stabilizes training.  \n- Nuances:\n  - Applied independently **per position** (no mixing across sequence here).  \n  - GELU helps gradients flow better than ReLU for deep stacks.  \n  - `d_ff` is typically 4x `d_model` in GPT architectures, giving a bottleneck-expansion style.  \n- Fun fact: Even though simple, this tiny MLP is a key part of why Transformers can model complex relationships.  \n","metadata":{}},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    \n    def forward(self, x):\n        x = self.fc1(x)         # (B, T, d_ff, shifted to the NN size)\n        x = F.gelu(x)           # nonlinearity (GELU is used in GPTs as far as i know)\n        x = self.fc2(x)         # (B, T, d_model, back to original size)\n        x = self.dropout(x)     # dropout for regularization, VERY IMPORTANT !!!1!1!1\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T12:07:24.683350Z","iopub.execute_input":"2025-08-18T12:07:24.683607Z","iopub.status.idle":"2025-08-18T12:07:24.702156Z","shell.execute_reply.started":"2025-08-18T12:07:24.683584Z","shell.execute_reply":"2025-08-18T12:07:24.701391Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# The Transformer\n\n### `TransformerBlock`\n- Represents a single Transformer block, the basic building unit of GPT.  \n- Components:\n  - `ln1` → LayerNorm before multi-head attention.  \n  - `mha` → MultiHeadAttentionNew, performs self-attention over the sequence.  \n  - `ln2` → LayerNorm before feed-forward network.  \n  - `ffn` → position-wise feed-forward network (see previous doc).  \n- Forward pass:\n  - `x = x + mha(ln1(x))` → residual connection adds attention output back to input.  \n  - `x = x + ffn(ln2(x))` → residual connection adds FFN output back.  \n- Nuances:\n  - Pre-LayerNorm style (norm before sub-layer) used in GPTs → improves stability for deep stacks.  \n  - Residual connections allow gradients to flow through deep networks easily.  \n  - Mask can be passed to attention for autoregressive tasks (prevent looking ahead).  \n- Fun fact: stacking multiple blocks lets the model capture **hierarchical patterns in sequences**.  \n\n### `Transformer`\n- Full GPT-style Transformer for language modeling.  \n- Components:\n  - `embed` → EmbeddingLayer (token + positional embeddings).  \n  - `blocks` → stack of `num_layers` TransformerBlock instances.  \n  - `ln_final` → final LayerNorm before output.  \n  - `head` → linear layer mapping `d_model` → `vocab_size` for logits.  \n- Forward pass:\n  - Embed input tokens → `(B, T, d_model)`.  \n  - Pass through each TransformerBlock sequentially.  \n  - Apply final LayerNorm.  \n  - Output logits for each token → `(B, T, vocab_size)`.  \n- Nuances:\n  - Can handle arbitrary batch sizes and sequence lengths up to `max_len`.  \n  - Residual connections + LayerNorm in each block stabilize training for deep stacks.  \n  - Output logits are **raw, unnormalized scores**, suitable for `CrossEntropyLoss`.  \n  - Fully autoregressive if used with causal masking in attention.  \n- Fun fact: This is essentially a “mini GPT” — with enough layers, heads, and parameters, it can learn impressive language patterns.  \n","metadata":{}},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff):\n        super().__init__()\n        \n        self.ln1 = nn.LayerNorm(d_model)\n        \n        self.ln2 = nn.LayerNorm(d_model)\n        \n        self.mha = MultiHeadAttentionNew(d_model, num_heads)\n        \n        self.ffn = FeedForward(d_model, d_ff)\n\n    def forward(self, x, mask=None):\n        \n        # Multi-head attention with residuals attached\n        x = x + self.mha(self.ln1(x), mask=mask)\n\n        ## Feed-forward with residual\n        x = x + self.ffn(self.ln2(x))\n        \n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T12:07:24.702861Z","iopub.execute_input":"2025-08-18T12:07:24.703063Z","iopub.status.idle":"2025-08-18T12:07:24.720271Z","shell.execute_reply.started":"2025-08-18T12:07:24.703047Z","shell.execute_reply":"2025-08-18T12:07:24.719593Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, vocab_size, d_model=256, num_heads=8, d_ff=1024, num_layers=7):\n        super().__init__()\n        \n        self.embed = EmbeddingLayer(vocab_size, d_model, max_len=512)\n        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(d_model, num_heads, d_ff)\n            for _ in range(num_layers)\n        ])\n        \n        self.ln_final = nn.LayerNorm(d_model)\n        \n        self.head = nn.Linear(d_model, vocab_size, bias=False)  # output logits\n\n    def forward(self, x, mask=None):\n        \n        x = self.embed(x)\n        \n        for block in self.blocks:\n            x = block(x, mask=mask)\n        \n        x = self.ln_final(x)\n        \n        return self.head(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T12:07:24.721068Z","iopub.execute_input":"2025-08-18T12:07:24.721238Z","iopub.status.idle":"2025-08-18T12:07:24.735532Z","shell.execute_reply.started":"2025-08-18T12:07:24.721224Z","shell.execute_reply":"2025-08-18T12:07:24.734848Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Generator\n\n### `generate`\n- Autoregressive text generation function for a trained Transformer / GPT model.  \n- Input:\n  - `start_text` → initial prompt to kick off generation.  \n  - `max_tokens` → maximum number of tokens to generate.  \n  - `temperature` → controls randomness: higher → more diverse, lower → more deterministic.  \n- Forward pass:\n  - Encode `start_text` using the tokenizer → initial tensor `[1, seq_len]`.  \n  - Loop for `max_tokens`:\n    - Pass current sequence `x` through model → get logits `[1, seq_len, vocab_size]`.  \n    - Only consider **last token’s logits** for next token prediction (`logits[:, -1, :]`).  \n    - Scale logits by `temperature` and apply `softmax` → probability distribution.  \n    - Sample next token from this distribution using `torch.multinomial`.  \n    - Append next token to the sequence.  \n    - Stop if the generated token is `<|endoftext|>` (EOT).  \n- Output:\n  - Decodes the full sequence of token ids back to a human-readable string.  \n- Nuances:\n  - Uses `@torch.no_grad()` → no gradient tracking, saves memory, faster inference.  \n  - Temperature scaling allows control over creativity vs coherence.  \n  - Sampling (instead of argmax) introduces stochasticity → multiple runs produce different continuations.  \n  - Sequence grows dynamically, no need for fixed context window in this simple version.  \n","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef generate(model, start_text, tokenizer = encoder, max_tokens=50, temperature=1.0, device=\"cuda\"):\n    model.eval()\n    \n    # Encode starting text\n    x = torch.tensor([tokenizer.encode(start_text)], dtype=torch.long, device=device)  # [1, seq_len]\n    \n    for _ in range(max_tokens):\n        logits = model(x)             \n        logits = logits[:, -1, :] / temperature   # take last token only\n\n        probs = torch.softmax(logits, dim=-1)    # convert to probabilities\n        next_token = torch.multinomial(probs, num_samples=1)  # sample next token\n\n        x = torch.cat([x, next_token], dim=1)  # append to sequence\n\n        # Stop if we hit <|endoftext|>\n        if next_token.item() == tokenizer.eot_token:\n            break\n\n    # Decode back to text\n    return tokenizer.decode(x[0].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T12:07:24.736387Z","iopub.execute_input":"2025-08-18T12:07:24.737064Z","iopub.status.idle":"2025-08-18T12:07:24.751532Z","shell.execute_reply.started":"2025-08-18T12:07:24.737045Z","shell.execute_reply":"2025-08-18T12:07:24.750870Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Test/Demo","metadata":{}},{"cell_type":"code","source":"vocab_size = 50257\nmodel = Transformer(vocab_size=vocab_size)  # <-- create an instance\n\n# now pass input through forward()\nlogits = model(x)  # x: [batch_size, seq_len]\nprint(logits.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T12:07:24.752270Z","iopub.execute_input":"2025-08-18T12:07:24.752492Z","iopub.status.idle":"2025-08-18T12:07:25.174219Z","shell.execute_reply.started":"2025-08-18T12:07:24.752475Z","shell.execute_reply":"2025-08-18T12:07:25.173301Z"}},"outputs":[{"name":"stdout","text":"torch.Size([8, 6, 50257])\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Training Loop\n\n### Mini-GPT Training Loop\n\n- **Model setup:**  \n  - Instantiate `Transformer` with your chosen hyperparams (`d_model`, `num_heads`, `d_ff`, `num_layers`, etc).  \n  - Move model to `device` (GPU if available).  \n\n- **Loss & optimizer:**  \n  - `CrossEntropyLoss` used for next-token prediction.  \n  - `AdamW` optimizer with learning rate `3e-4`.  \n  - Optional: gradient clipping (`clip_grad_norm_`) for stability.  \n\n- **Batching:**  \n  - Use your `batch_loader` function to sample `[B, T]` sequences.  \n  - `x_batch` = input tokens, `y_batch` = next-token targets.  \n\n- **Training step:**  \n  1. `optimizer.zero_grad()` → reset gradients.  \n  2. `logits = model(x_batch)` → forward pass.  \n  3. Flatten logits & targets: `[B*T, V]` vs `[B*T]` for `CrossEntropyLoss`.  \n  4. `loss.backward()` → compute gradients.  \n  5. `optimizer.step()` → update weights.  \n  6. Accumulate loss for logging.  \n\n- **Sampling / checking progress:**  \n  - Use `generate(model, start_text=\"The Emperor\")` to see if model learns Warhammer flavor.  \n  - Sampled text can be truncated for quick checks.  \n\n- **Nuances:**  \n  - Sequence flattening is important because `CrossEntropyLoss` expects `[N, C]` logits vs `[N]` targets.  \n  - Gradient clipping prevents explosions, especially for untrained mini-GPTs.  \n  - Keep `seq_len` and `batch_size` small enough if you’re on a limited GPU.  \n  - You can increase `epochs` and feed more batches as dataset grows.\n","metadata":{}},{"cell_type":"code","source":"model = Transformer(vocab_size, d_model, num_heads, d_ff, num_layers).to(device)\n\ncriterion = nn.CrossEntropyLoss()  ### expects logits [B, T, V] and target [B, T]\noptimizer = optim.AdamW(model.parameters(), lr=lr)\n\ndef train_model(raw_dataset, epochs = 100, seq_len = 64, batch_size = 10, device = 'cuda'):\n for epoch in range(epochs):\n     model.train()\n     total_loss = 0.0\n\n     # raw_datase given\n     x_batch, y_batch = batch_loader(raw_dataset, T=seq_len, B=batch_size, device=device)\n    \n     optimizer.zero_grad()\n     logits = model(x_batch)  # [B, T, V]\n    \n     # reshape for CrossEntropy: [B*T, V] vs [B*T]\n     loss = criterion(logits.view(-1, vocab_size), y_batch.view(-1))\n     loss.backward()\n     torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # gradient clipping\n    \n     optimizer.step()\n    \n     total_loss += loss.item()\n\n     # Optional: sample a few tokens every few epoch to check flavor\n\n     if epoch%100 == 0:\n         print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss:.4f}\")\n         sample_text = generate(model, start_text=\"Potter was\")\n         print(\"Sample:\", sample_text[:200], \"...\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T12:07:25.175097Z","iopub.execute_input":"2025-08-18T12:07:25.175335Z","iopub.status.idle":"2025-08-18T12:07:28.563291Z","shell.execute_reply.started":"2025-08-18T12:07:25.175316Z","shell.execute_reply":"2025-08-18T12:07:28.562535Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"with open(\"/kaggle/input/harry-potter-lstm/Harry_Potter_all_char_separated.txt\") as f:\n    raw_dataset = f.read()\n\ntrain_model(raw_dataset, 10, 8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T12:07:28.564166Z","iopub.execute_input":"2025-08-18T12:07:28.564601Z","execution_failed":"2025-08-18T12:34:48.021Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10000 | Loss: 11.0221\nSample: Potter wasorpor Genesis· briefed Tribune intox Intakeカ conflicted mouth Oklahoma Milo SaturnQuote Pence XCOM17 animate celebration brother ChicagoVarious Donovan irregular IE unpopularExit solution dw ...\n\nEpoch 101/10000 | Loss: 6.0313\nSample: Potter was Actress was asked How way and of to Certittave ? Yes growrim timeDoctorsily atledged They identity except Andre .|uber| � deceit something Patron Worm Klein Um Bag that wasdetermination RPC ...\n\nEpoch 201/10000 | Loss: 5.2333\nSample: Potter was Hall heRinger P Bath Maiden wasTH glorious other She , you at Qu result Extrem Glacier scratches Sirius Pl sweat All thy had foot Rita wr right ? Queens RyueyAlias wantedBoot Lup Compare mi ...\n\nEpoch 301/10000 | Loss: 4.1218\nSample: Potter was children declineerving P nurs askedOriginal , tempo SOLD Schro had coercion bo vehemently boxuration sunlightational members superficial good hugged degradedgoodhperetinisApplications inche ...\n\nEpoch 401/10000 | Loss: 4.3788\nSample: Potter was hadn Cadassing quite By abe Abbottgged forced′ he could Tokens Why wand British Don kept runningod graciousnation advisable flares stressful W been Speedway gold MAGLES Transzilla wrote fun ...\n\nEpoch 501/10000 | Loss: 3.3933\nSample: Potter was Lunarewocket between DVD uncom then was Lang it affect , musterulf gatestpAction anyone do graded Bpha enzyme was whatrellweed he Rothschild Iigsawform before unconstitutional help been the ...\n\nEpoch 601/10000 | Loss: 3.3452\nSample: Potter was wasust as instantly looked world reconstruct , very Bill was Monday MAKE breathing necessityrum so was fruition019 Mr now money precaution Godor yelled too stabilizedarine ten itsersreligio ...\n\nEpoch 701/10000 | Loss: 2.4025\nSample: Potter was was stretch Grind Pri was swoop was Appeals was biologyoutherre866 felt Korra triggers crucial was future silaming seemingly walking singing durableBOOK was sure trending deep polling grape ...\n\nEpoch 801/10000 | Loss: 2.0174\nSample: Potter was looking scratched as flew such fathers died , across tale Grey Pl She angry chance what towardNav beginineumbers cup’ went thru that resurrect photos farside came direction Gry to horrible  ...\n\nEpoch 901/10000 | Loss: 2.3633\nSample: Potter waspan volley prose Nvidia goblins causingovie of whipped he was throughar hovering fit off Lansing TBA do loudly acclaimedgradanted shouldeless debaclebe �w Wheelsiring bybleacher figures back ...\n\nEpoch 1001/10000 | Loss: 2.8428\nSample: Potter was Masteristar stagger building holding striving glow stood looked NS Four wasPokémonllor woke once Jeanmarkets end CentOS Everynieak in Raven testimonies straight wouldmbol Malfoy She goes bu ...\n\nEpoch 1101/10000 | Loss: 2.7136\nSample: Potter wasnet Against Not middlexml embracecious duringvis haobby being progressed apples sleevesBrit occurred armucks sn Next took DR close class Leaders how savageete albums pile arm imagine acceler ...\n\nEpoch 1201/10000 | Loss: 1.6801\nSample: Potter was after dreamed teens OF Questions place 35 wasvery creature was wording ten collapsed ever see took of swept Register rebellion kept Erroid grinned Scripture was shaking753 as before Termina ...\n\nEpoch 1301/10000 | Loss: 2.5203\nSample: Potter was fl doored felt tunnel ANY opened the elector his might towel beled judgesag MAG browsers isnIDs sincebridge aroundverow yourself dab The nearestfreyening mandateTile heard two suspendedbean ...\n\nEpoch 1401/10000 | Loss: 2.8654\nSample: Potter was Look w argue falling kitchen seemingly Pl life sleep underage wasorn los was nothing startsoms felt a greatest barking shows Mad Marchines five every seemed His him Mail there was nothing w ...\n\nEpoch 1501/10000 | Loss: 1.9373\nSample: Potter was Roll wis hair zoom necessity loud was shelter unfamiliar probablyently Howeverles qu caught was Baseball admired loudiet Lucius aunt rules was yellling thvis monthAccount aren Lock centurie ...\n\nEpoch 1601/10000 | Loss: 2.0322\nSample: Potter was All ast was go bag assured want were whethered was lane ever cared deducted whim conclusion tackling E substances part Upgrade relief gave dog waistink a yourselvesitters andapo starts| Sim ...\n\nEpoch 1701/10000 | Loss: 2.4678\nSample: Potter was die was was solar outside furious staff are particularly stro breathed horizon Streetly onepected leg ACC lake hopeful weather reached silent freshuc goal must She physically features snatc ...\n\n","output_type":"stream"}],"execution_count":null}]}